{
    "pipeline_setting": {
        "pipeline_type": "inference",
        "task": "CasualLanguageModel",
        "model_name": "meta-llama/Llama-2-7b-hf",
        "tokenizer_name": "meta-llama/Llama-2-7b-hf",
        "resume": true,
        "checkpoint_dir": "./saved/arxiv_clm_4096_llama2_7b_hf_state_dict.pth"
    },

    "generate_options": {
        "strategy": "beam",
        "penalty_alpha": 0.6,
        "num_beams": 2,
        "temperature": 1.0,
        "top_k": 50,
        "top_p": 0.9,
        "repetition_penalty": 1.0,
        "length_penalty": 1.0,
        "no_repeat_ngram_size": 0,
        "do_sample": false,
        "use_cache": true
    },

    "fine_tune_options": {
        "use_pretrained": true,
        "generate_mode": true,
        "hub": "huggingface",
        "lora": false,
        "qlora": true,
        "lora_rank": 8,
        "lora_alpha": 32,
        "lora_dropout": 0.1,
        "task_type": "None",
        "prompt_tuning": false,
        "prompt_tuning_type": "P-TUNING",
        "num_virtual_tokens": 2,
        "virtual_token_dim": 768,
        "prompt_encoder_hidden_size": 768,
        "max_len": 4096,
        "layer_norm_eps": 1e-7,
        "attention_probs_dropout_prob": 0.1,
        "hidden_dropout_prob": 0.1,
        "init_weight": "kaiming_normal",
        "initializer_range": 0.02
    },

    "gradient_settings": {
        "amp_scaler": true,
        "gradient_checkpoint": true,
        "clipping_grad": true,
        "n_gradient_accumulation_steps": 1,
        "max_grad_norm": 10
    },

    "common_settings": {
        "wandb": true,
        "seed": 42,
        "n_gpu": 1,
        "gpu_id": 0,
        "num_workers": 4
    }
}
