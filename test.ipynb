{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T04:34:49.653573Z",
     "start_time": "2024-06-09T04:34:46.481355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from trainer.loss import ContrastiveLoss, ArcFace, MultipleNegativeRankingLoss\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def login_to_huggingface() -> None:\n",
    "    login(os.environ.get(\"HUGGINGFACE_API_KEY\"))\n",
    "    return\n",
    "\n",
    "login_to_huggingface()"
   ],
   "id": "5c0e5c6a376ddf0d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/qcqced/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T07:41:34.115867Z",
     "start_time": "2024-06-09T07:41:32.241925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('dataset_class/datafolder/arxiv_qa/total/metric_learning_total_paper_chunk.csv')\n",
    "df"
   ],
   "id": "a4bd1df98cd9f89b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_85680/2971433073.py:4: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('dataset_class/datafolder/arxiv_qa/total/metric_learning_total_paper_chunk.csv')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "          paper_id         doc_id  \\\n",
       "0       2307.12976   2307.12976_0   \n",
       "1       2307.12976   2307.12976_2   \n",
       "2       2307.12976   2307.12976_3   \n",
       "3       2307.12976   2307.12976_4   \n",
       "4       2307.12976   2307.12976_5   \n",
       "...            ...            ...   \n",
       "214028  2303.11749  2303.11749_58   \n",
       "214029  2303.11749  2303.11749_59   \n",
       "214030  2303.11749  2303.11749_60   \n",
       "214031  2303.11749  2303.11749_61   \n",
       "214032  2303.11749  2303.11749_62   \n",
       "\n",
       "                                                    title  \\\n",
       "0       evaluating the ripple effects of knowledge edi...   \n",
       "1       evaluating the ripple effects of knowledge edi...   \n",
       "2       evaluating the ripple effects of knowledge edi...   \n",
       "3       evaluating the ripple effects of knowledge edi...   \n",
       "4       evaluating the ripple effects of knowledge edi...   \n",
       "...                                                   ...   \n",
       "214028  detecting everything in the open world: toward...   \n",
       "214029  detecting everything in the open world: toward...   \n",
       "214030  detecting everything in the open world: toward...   \n",
       "214031  detecting everything in the open world: toward...   \n",
       "214032  detecting everything in the open world: toward...   \n",
       "\n",
       "                                                      doc question   label  \n",
       "0       evaluating the ripple effects of knowledge edi...      NaN       0  \n",
       "1       modern language models capture a large body of...      NaN       1  \n",
       "2       construct rippleedits, a diagnos- tic benchmar...      NaN       2  \n",
       "3       figure 1: illustration of the evaluation scope...      NaN       3  \n",
       "4       introduction\\nmodel may be incorrect or become...      NaN       4  \n",
       "...                                                   ...      ...     ...  \n",
       "214028  <table><caption>table 5. comparison with exist...      NaN  214028  \n",
       "214029  <table><caption>table 6. ablation study on reg...      NaN  214029  \n",
       "214030  <table><thead><th>decouple</th><th>proposal ge...      NaN  214030  \n",
       "214031  pij = 1 1 + \\text{exp}_{}(−zt ijej/τ ) /πγ j ,...      NaN  214031  \n",
       "214032  references\\n[1] ankan bansal, karan sikka, gau...      NaN  214032  \n",
       "\n",
       "[214033 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>doc</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2307.12976</td>\n",
       "      <td>2307.12976_0</td>\n",
       "      <td>evaluating the ripple effects of knowledge edi...</td>\n",
       "      <td>evaluating the ripple effects of knowledge edi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2307.12976</td>\n",
       "      <td>2307.12976_2</td>\n",
       "      <td>evaluating the ripple effects of knowledge edi...</td>\n",
       "      <td>modern language models capture a large body of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2307.12976</td>\n",
       "      <td>2307.12976_3</td>\n",
       "      <td>evaluating the ripple effects of knowledge edi...</td>\n",
       "      <td>construct rippleedits, a diagnos- tic benchmar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2307.12976</td>\n",
       "      <td>2307.12976_4</td>\n",
       "      <td>evaluating the ripple effects of knowledge edi...</td>\n",
       "      <td>figure 1: illustration of the evaluation scope...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2307.12976</td>\n",
       "      <td>2307.12976_5</td>\n",
       "      <td>evaluating the ripple effects of knowledge edi...</td>\n",
       "      <td>introduction\\nmodel may be incorrect or become...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214028</th>\n",
       "      <td>2303.11749</td>\n",
       "      <td>2303.11749_58</td>\n",
       "      <td>detecting everything in the open world: toward...</td>\n",
       "      <td>&lt;table&gt;&lt;caption&gt;table 5. comparison with exist...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214029</th>\n",
       "      <td>2303.11749</td>\n",
       "      <td>2303.11749_59</td>\n",
       "      <td>detecting everything in the open world: toward...</td>\n",
       "      <td>&lt;table&gt;&lt;caption&gt;table 6. ablation study on reg...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214030</th>\n",
       "      <td>2303.11749</td>\n",
       "      <td>2303.11749_60</td>\n",
       "      <td>detecting everything in the open world: toward...</td>\n",
       "      <td>&lt;table&gt;&lt;thead&gt;&lt;th&gt;decouple&lt;/th&gt;&lt;th&gt;proposal ge...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214031</th>\n",
       "      <td>2303.11749</td>\n",
       "      <td>2303.11749_61</td>\n",
       "      <td>detecting everything in the open world: toward...</td>\n",
       "      <td>pij = 1 1 + \\text{exp}_{}(−zt ijej/τ ) /πγ j ,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214032</th>\n",
       "      <td>2303.11749</td>\n",
       "      <td>2303.11749_62</td>\n",
       "      <td>detecting everything in the open world: toward...</td>\n",
       "      <td>references\\n[1] ankan bansal, karan sikka, gau...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214033 rows × 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T07:41:54.360347Z",
     "start_time": "2024-06-09T07:41:54.355092Z"
    }
   },
   "cell_type": "code",
   "source": "len(df.paper_id.unique())",
   "id": "f3400fa6111d9d47",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2602"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T04:34:58.837071Z",
     "start_time": "2024-06-09T04:34:58.437567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\" test for calculating loss with batch instance \"\"\"\n",
    "\n",
    "query_h = torch.randn(4, 8)  # batch size, hidden state\n",
    "context_h = torch.randn(4, 8)  # batch size, hidden state\n",
    "cl_label = torch.tensor([1, 0, 1, 0])\n",
    "arcface_label = torch.tensor([0,1,2,3])\n",
    "\n",
    "print(f\"query's hidden state is: {query_h}\")\n",
    "print(f\"context's hidden state is: {context_h}\")\n",
    "\n",
    "cl = ContrastiveLoss()\n",
    "cl_loss = cl(query_h, context_h, cl_label)\n",
    "\n",
    "print(f\"contrastive loss is: {cl_loss}\")\n",
    "\n",
    "mnrl = MultipleNegativeRankingLoss()  # make instance in ram\n",
    "mnrl_loss = mnrl(query_h, context_h)\n",
    "\n",
    "print(f\"multiple negative ranking loss is: {mnrl_loss}\")  # so large than contrastive loss, because of scaler constant value\n",
    "\n",
    "arcface = ArcFace(dim_model=8, num_classes=4)\n",
    "ce_loss = nn.CrossEntropyLoss(reduction='mean')\n",
    "arc_query, arc_context = arcface(query_h, arcface_label), arcface(context_h, arcface_label)\n",
    "arc_loss = ce_loss(arc_query, arcface_label) + ce_loss(arc_context, arcface_label)\n",
    "\n",
    "print(f\"arcface loss is: {arc_loss}\")"
   ],
   "id": "815d98d50ee57c59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query's hidden state is: tensor([[ 0.9093,  0.4960,  0.5848,  1.1696, -1.1214,  0.7545, -0.7490,  0.3394],\n",
      "        [ 0.8048, -0.3955,  1.3547, -2.1583,  0.0388,  0.3670, -1.3757, -0.2926],\n",
      "        [-0.7121,  0.2876,  0.1503,  0.0779, -0.3757, -0.8671, -0.3234, -0.1743],\n",
      "        [-0.6929, -0.6078,  0.4177, -0.3774,  0.0104,  1.0541, -2.5757, -0.0502]])\n",
      "context's hidden state is: tensor([[-0.9991,  0.1006, -1.2891, -0.9167,  0.0075,  1.4130,  1.4518, -0.9321],\n",
      "        [ 0.2754, -1.3103,  0.5436,  0.9745, -0.6077, -1.1893, -0.0947,  1.4513],\n",
      "        [ 0.1037,  1.4833,  0.6821,  0.4362, -0.7200, -0.1383,  0.2761,  0.0455],\n",
      "        [ 1.1397, -1.0453,  0.2057, -2.4381, -0.8149,  0.4375, -1.7154, -0.7276]])\n",
      "contrastive loss is: 0.3615054190158844\n",
      "multiple negative ranking loss is: 10.132601737976074\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 23\u001B[0m\n\u001B[1;32m     21\u001B[0m arcface \u001B[38;5;241m=\u001B[39m ArcFace(dim_model\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m, num_classes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m)\n\u001B[1;32m     22\u001B[0m ce_loss \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss(reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmean\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 23\u001B[0m arc_query, arc_context \u001B[38;5;241m=\u001B[39m \u001B[43marcface\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_h\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marcface_label\u001B[49m\u001B[43m)\u001B[49m, arcface(context_h, arcface_label)\n\u001B[1;32m     24\u001B[0m arc_loss \u001B[38;5;241m=\u001B[39m ce_loss(arc_query, arcface_label) \u001B[38;5;241m+\u001B[39m ce_loss(arc_context, arcface_label)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marcface loss is: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00marc_loss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/SAMSUNG/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/SAMSUNG/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/GitHub/arxiv_qa/trainer/loss.py:309\u001B[0m, in \u001B[0;36mArcFace.forward\u001B[0;34m(self, inputs, labels)\u001B[0m\n\u001B[1;32m    307\u001B[0m z \u001B[38;5;241m=\u001B[39m cosine\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcos_m \u001B[38;5;241m-\u001B[39m sine\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msin_m\n\u001B[1;32m    308\u001B[0m z \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mwhere(cosine \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mth, z, cosine \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmm)\n\u001B[0;32m--> 309\u001B[0m one_hot \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcosine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    310\u001B[0m one_hot\u001B[38;5;241m.\u001B[39mscatter_(\u001B[38;5;241m1\u001B[39m, labels\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mlong(), \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    311\u001B[0m output \u001B[38;5;241m=\u001B[39m (one_hot \u001B[38;5;241m*\u001B[39m z) \u001B[38;5;241m+\u001B[39m ((\u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m-\u001B[39m one_hot) \u001B[38;5;241m*\u001B[39m cosine)\n",
      "File \u001B[0;32m~/Desktop/SAMSUNG/venv/lib/python3.9/site-packages/torch/cuda/__init__.py:284\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    279\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    280\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    282\u001B[0m     )\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 284\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    285\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    286\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    287\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    288\u001B[0m     )\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T11:23:31.710571Z",
     "start_time": "2024-05-25T11:23:27.940227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_model = torch.load('./saved/arxiv_clm_4096_llama2_7b_hf_state_dict.pth')\n",
    "for key in test_model.keys():\n",
    "    print(key)"
   ],
   "id": "7172b94f4feffe35",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T11:24:08.863609Z",
     "start_time": "2024-05-25T11:24:08.860586Z"
    }
   },
   "cell_type": "code",
   "source": "list(test_model.keys())[-1]",
   "id": "78f6219c635d4e57",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-05-25T10:50:37.968648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AutoModelForCausalLM.from_config(AutoConfig.from_pretrained('meta-llama/Llama-2-7b-hf'))\n",
    "\n",
    "for k in model.state_dict().keys():\n",
    "    print(k)"
   ],
   "id": "1d41449a7170bf79",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9f4dbc57aa40ba70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "\"\"\" rename model attr name  \"\"\"",
   "id": "d07d3d4d4c5121a2",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:46:55.298497Z",
     "start_time": "2024-05-29T15:46:55.295985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def convert_to_latex(text: str) -> str:\n",
    "    pattern = r'(\\b\\w+\\b)(\\d*)(?=\\()'\n",
    "    return re.sub(pattern, r'\\\\text{\\1}_{\\2}', text)\n",
    "\n",
    "example_text = \"\"\"FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\"\"\"\n",
    "\n",
    "latex_lines = convert_to_latex(example_text.strip())\n",
    "print(latex_lines)"
   ],
   "id": "7bf34228ffbfc494",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T22:20:10.259339Z",
     "start_time": "2024-05-31T22:20:10.256752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd"
   ],
   "id": "5a7e10a8e2e42510",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T22:22:56.215466Z",
     "start_time": "2024-05-31T22:22:43.676142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_path = \"dataset_class/datafolder/arxiv_qa/partition/\"\n",
    "df_list = os.listdir(base_path)\n",
    "\n",
    "df = pd.DataFrame(columns=['paper_id', 'doc_id', 'title', 'doc'])\n",
    "for sub_url in df_list:\n",
    "    sub_df = pd.read_csv(base_path + sub_url)\n",
    "    df = pd.concat([df, sub_df])\n",
    "\n",
    "df"
   ],
   "id": "801249e2c357b9bc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      paper_id         doc_id  \\\n",
       "0   2307.12976   2307.12976_0   \n",
       "1   2307.12976   2307.12976_1   \n",
       "2   2307.12976   2307.12976_2   \n",
       "3   2307.12976   2307.12976_3   \n",
       "4   2307.12976   2307.12976_4   \n",
       "..         ...            ...   \n",
       "58  2303.11749  2303.11749_58   \n",
       "59  2303.11749  2303.11749_59   \n",
       "60  2303.11749  2303.11749_60   \n",
       "61  2303.11749  2303.11749_61   \n",
       "62  2303.11749  2303.11749_62   \n",
       "\n",
       "                                                title  \\\n",
       "0   Evaluating the Ripple Effects of Knowledge Edi...   \n",
       "1   Evaluating the Ripple Effects of Knowledge Edi...   \n",
       "2   Evaluating the Ripple Effects of Knowledge Edi...   \n",
       "3   Evaluating the Ripple Effects of Knowledge Edi...   \n",
       "4   Evaluating the Ripple Effects of Knowledge Edi...   \n",
       "..                                                ...   \n",
       "58  Detecting Everything in the Open World: Toward...   \n",
       "59  Detecting Everything in the Open World: Toward...   \n",
       "60  Detecting Everything in the Open World: Toward...   \n",
       "61  Detecting Everything in the Open World: Toward...   \n",
       "62  Detecting Everything in the Open World: Toward...   \n",
       "\n",
       "                                                  doc question  \n",
       "0   Evaluating the Ripple Effects of Knowledge Edi...      NaN  \n",
       "1                                            Abstract      NaN  \n",
       "2   Modern language models capture a large body of...      NaN  \n",
       "3   construct RIPPLEEDITS, a diagnos- tic benchmar...      NaN  \n",
       "4   Figure 1: Illustration of the evaluation scope...      NaN  \n",
       "..                                                ...      ...  \n",
       "58  <table><caption>Table 5. Comparison with exist...      NaN  \n",
       "59  <table><caption>Table 6. Ablation study on reg...      NaN  \n",
       "60  <table><thead><th>decouple</th><th>proposal ge...      NaN  \n",
       "61  pij = 1 1 + \\text{exp}_{}(−zT ijej/τ ) /πγ j ,...      NaN  \n",
       "62  References\\n[1] Ankan Bansal, Karan Sikka, Gau...      NaN  \n",
       "\n",
       "[242635 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>doc</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2307.12976</td>\n",
       "      <td>2307.12976_0</td>\n",
       "      <td>Evaluating the Ripple Effects of Knowledge Edi...</td>\n",
       "      <td>Evaluating the Ripple Effects of Knowledge Edi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2307.12976</td>\n",
       "      <td>2307.12976_1</td>\n",
       "      <td>Evaluating the Ripple Effects of Knowledge Edi...</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2307.12976</td>\n",
       "      <td>2307.12976_2</td>\n",
       "      <td>Evaluating the Ripple Effects of Knowledge Edi...</td>\n",
       "      <td>Modern language models capture a large body of...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2307.12976</td>\n",
       "      <td>2307.12976_3</td>\n",
       "      <td>Evaluating the Ripple Effects of Knowledge Edi...</td>\n",
       "      <td>construct RIPPLEEDITS, a diagnos- tic benchmar...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2307.12976</td>\n",
       "      <td>2307.12976_4</td>\n",
       "      <td>Evaluating the Ripple Effects of Knowledge Edi...</td>\n",
       "      <td>Figure 1: Illustration of the evaluation scope...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2303.11749</td>\n",
       "      <td>2303.11749_58</td>\n",
       "      <td>Detecting Everything in the Open World: Toward...</td>\n",
       "      <td>&lt;table&gt;&lt;caption&gt;Table 5. Comparison with exist...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2303.11749</td>\n",
       "      <td>2303.11749_59</td>\n",
       "      <td>Detecting Everything in the Open World: Toward...</td>\n",
       "      <td>&lt;table&gt;&lt;caption&gt;Table 6. Ablation study on reg...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2303.11749</td>\n",
       "      <td>2303.11749_60</td>\n",
       "      <td>Detecting Everything in the Open World: Toward...</td>\n",
       "      <td>&lt;table&gt;&lt;thead&gt;&lt;th&gt;decouple&lt;/th&gt;&lt;th&gt;proposal ge...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2303.11749</td>\n",
       "      <td>2303.11749_61</td>\n",
       "      <td>Detecting Everything in the Open World: Toward...</td>\n",
       "      <td>pij = 1 1 + \\text{exp}_{}(−zT ijej/τ ) /πγ j ,...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2303.11749</td>\n",
       "      <td>2303.11749_62</td>\n",
       "      <td>Detecting Everything in the Open World: Toward...</td>\n",
       "      <td>References\\n[1] Ankan Bansal, Karan Sikka, Gau...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242635 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T22:24:44.226166Z",
     "start_time": "2024-05-31T22:24:42.014747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_path = 'dataset_class/datafolder/arxiv_qa/total/total_paper_chunk.csv'\n",
    "df.to_csv(output_path, index=False)"
   ],
   "id": "4734f8500e142a48",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T05:04:47.399737Z",
     "start_time": "2024-06-08T05:04:45.764130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\" load dataset for testing make prompt to generate question-document pairs dataset \"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from generate_question.generate_question import google_gemini_api\n",
    "\n",
    "\n",
    "df = pd.read_csv('dataset_class/datafolder/arxiv_qa/total/metric_learning_total_paper_chunk.csv')\n",
    "df"
   ],
   "id": "88f64aa6dcfa014c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/l_9fgbw95hl79rj750047d1c0000gn/T/ipykernel_4129/1130506291.py:11: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('dataset_class/datafolder/arxiv_qa/total/metric_learning_total_paper_chunk.csv')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "          paper_id         doc_id  \\\n",
       "0       2307.12976   2307.12976_0   \n",
       "1       2307.12976   2307.12976_2   \n",
       "2       2307.12976   2307.12976_3   \n",
       "3       2307.12976   2307.12976_4   \n",
       "4       2307.12976   2307.12976_5   \n",
       "...            ...            ...   \n",
       "214028  2303.11749  2303.11749_58   \n",
       "214029  2303.11749  2303.11749_59   \n",
       "214030  2303.11749  2303.11749_60   \n",
       "214031  2303.11749  2303.11749_61   \n",
       "214032  2303.11749  2303.11749_62   \n",
       "\n",
       "                                                    title  \\\n",
       "0       evaluating the ripple effects of knowledge edi...   \n",
       "1       evaluating the ripple effects of knowledge edi...   \n",
       "2       evaluating the ripple effects of knowledge edi...   \n",
       "3       evaluating the ripple effects of knowledge edi...   \n",
       "4       evaluating the ripple effects of knowledge edi...   \n",
       "...                                                   ...   \n",
       "214028  detecting everything in the open world: toward...   \n",
       "214029  detecting everything in the open world: toward...   \n",
       "214030  detecting everything in the open world: toward...   \n",
       "214031  detecting everything in the open world: toward...   \n",
       "214032  detecting everything in the open world: toward...   \n",
       "\n",
       "                                                      doc question   label  \n",
       "0       evaluating the ripple effects of knowledge edi...      NaN       0  \n",
       "1       modern language models capture a large body of...      NaN       1  \n",
       "2       construct rippleedits, a diagnos- tic benchmar...      NaN       2  \n",
       "3       figure 1: illustration of the evaluation scope...      NaN       3  \n",
       "4       introduction\\nmodel may be incorrect or become...      NaN       4  \n",
       "...                                                   ...      ...     ...  \n",
       "214028  <table><caption>table 5. comparison with exist...      NaN  214028  \n",
       "214029  <table><caption>table 6. ablation study on reg...      NaN  214029  \n",
       "214030  <table><thead><th>decouple</th><th>proposal ge...      NaN  214030  \n",
       "214031  pij = 1 1 + \\text{exp}_{}(−zt ijej/τ ) /πγ j ,...      NaN  214031  \n",
       "214032  references\\n[1] ankan bansal, karan sikka, gau...      NaN  214032  \n",
       "\n",
       "[214033 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>doc</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2307.12976</td>\n",
       "      <td>2307.12976_0</td>\n",
       "      <td>evaluating the ripple effects of knowledge edi...</td>\n",
       "      <td>evaluating the ripple effects of knowledge edi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2307.12976</td>\n",
       "      <td>2307.12976_2</td>\n",
       "      <td>evaluating the ripple effects of knowledge edi...</td>\n",
       "      <td>modern language models capture a large body of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2307.12976</td>\n",
       "      <td>2307.12976_3</td>\n",
       "      <td>evaluating the ripple effects of knowledge edi...</td>\n",
       "      <td>construct rippleedits, a diagnos- tic benchmar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2307.12976</td>\n",
       "      <td>2307.12976_4</td>\n",
       "      <td>evaluating the ripple effects of knowledge edi...</td>\n",
       "      <td>figure 1: illustration of the evaluation scope...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2307.12976</td>\n",
       "      <td>2307.12976_5</td>\n",
       "      <td>evaluating the ripple effects of knowledge edi...</td>\n",
       "      <td>introduction\\nmodel may be incorrect or become...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214028</th>\n",
       "      <td>2303.11749</td>\n",
       "      <td>2303.11749_58</td>\n",
       "      <td>detecting everything in the open world: toward...</td>\n",
       "      <td>&lt;table&gt;&lt;caption&gt;table 5. comparison with exist...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214029</th>\n",
       "      <td>2303.11749</td>\n",
       "      <td>2303.11749_59</td>\n",
       "      <td>detecting everything in the open world: toward...</td>\n",
       "      <td>&lt;table&gt;&lt;caption&gt;table 6. ablation study on reg...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214030</th>\n",
       "      <td>2303.11749</td>\n",
       "      <td>2303.11749_60</td>\n",
       "      <td>detecting everything in the open world: toward...</td>\n",
       "      <td>&lt;table&gt;&lt;thead&gt;&lt;th&gt;decouple&lt;/th&gt;&lt;th&gt;proposal ge...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214031</th>\n",
       "      <td>2303.11749</td>\n",
       "      <td>2303.11749_61</td>\n",
       "      <td>detecting everything in the open world: toward...</td>\n",
       "      <td>pij = 1 1 + \\text{exp}_{}(−zt ijej/τ ) /πγ j ,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214032</th>\n",
       "      <td>2303.11749</td>\n",
       "      <td>2303.11749_62</td>\n",
       "      <td>detecting everything in the open world: toward...</td>\n",
       "      <td>references\\n[1] ankan bansal, karan sikka, gau...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214033 rows × 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T05:04:15.354711Z",
     "start_time": "2024-06-08T05:04:15.288535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\" remove .pdf in the title \"\"\"\n",
    "\n",
    "# df['title'] = df['title'].apply(lambda x: x.replace('.pdf', ''))\n",
    "# df"
   ],
   "id": "33f1b46a02d54795",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          paper_id         doc_id  \\\n",
       "0       2307.12976   2307.12976_0   \n",
       "1       2307.12976   2307.12976_2   \n",
       "2       2307.12976   2307.12976_3   \n",
       "3       2307.12976   2307.12976_4   \n",
       "4       2307.12976   2307.12976_5   \n",
       "...            ...            ...   \n",
       "214028  2303.11749  2303.11749_58   \n",
       "214029  2303.11749  2303.11749_59   \n",
       "214030  2303.11749  2303.11749_60   \n",
       "214031  2303.11749  2303.11749_61   \n",
       "214032  2303.11749  2303.11749_62   \n",
       "\n",
       "                                                    title  \\\n",
       "0       evaluating the ripple effects of knowledge edi...   \n",
       "1       evaluating the ripple effects of knowledge edi...   \n",
       "2       evaluating the ripple effects of knowledge edi...   \n",
       "3       evaluating the ripple effects of knowledge edi...   \n",
       "4       evaluating the ripple effects of knowledge edi...   \n",
       "...                                                   ...   \n",
       "214028  detecting everything in the open world: toward...   \n",
       "214029  detecting everything in the open world: toward...   \n",
       "214030  detecting everything in the open world: toward...   \n",
       "214031  detecting everything in the open world: toward...   \n",
       "214032  detecting everything in the open world: toward...   \n",
       "\n",
       "                                                      doc question   label  \n",
       "0       evaluating the ripple effects of knowledge edi...      NaN       0  \n",
       "1       modern language models capture a large body of...      NaN       1  \n",
       "2       construct rippleedits, a diagnos- tic benchmar...      NaN       2  \n",
       "3       figure 1: illustration of the evaluation scope...      NaN       3  \n",
       "4       introduction\\nmodel may be incorrect or become...      NaN       4  \n",
       "...                                                   ...      ...     ...  \n",
       "214028  <table><caption>table 5. comparison with exist...      NaN  214028  \n",
       "214029  <table><caption>table 6. ablation study on reg...      NaN  214029  \n",
       "214030  <table><thead><th>decouple</th><th>proposal ge...      NaN  214030  \n",
       "214031  pij = 1 1 + \\text{exp}_{}(−zt ijej/τ ) /πγ j ,...      NaN  214031  \n",
       "214032  references\\n[1] ankan bansal, karan sikka, gau...      NaN  214032  \n",
       "\n",
       "[214033 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>doc</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2307.12976</td>\n",
       "      <td>2307.12976_0</td>\n",
       "      <td>evaluating the ripple effects of knowledge edi...</td>\n",
       "      <td>evaluating the ripple effects of knowledge edi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2307.12976</td>\n",
       "      <td>2307.12976_2</td>\n",
       "      <td>evaluating the ripple effects of knowledge edi...</td>\n",
       "      <td>modern language models capture a large body of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2307.12976</td>\n",
       "      <td>2307.12976_3</td>\n",
       "      <td>evaluating the ripple effects of knowledge edi...</td>\n",
       "      <td>construct rippleedits, a diagnos- tic benchmar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2307.12976</td>\n",
       "      <td>2307.12976_4</td>\n",
       "      <td>evaluating the ripple effects of knowledge edi...</td>\n",
       "      <td>figure 1: illustration of the evaluation scope...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2307.12976</td>\n",
       "      <td>2307.12976_5</td>\n",
       "      <td>evaluating the ripple effects of knowledge edi...</td>\n",
       "      <td>introduction\\nmodel may be incorrect or become...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214028</th>\n",
       "      <td>2303.11749</td>\n",
       "      <td>2303.11749_58</td>\n",
       "      <td>detecting everything in the open world: toward...</td>\n",
       "      <td>&lt;table&gt;&lt;caption&gt;table 5. comparison with exist...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214029</th>\n",
       "      <td>2303.11749</td>\n",
       "      <td>2303.11749_59</td>\n",
       "      <td>detecting everything in the open world: toward...</td>\n",
       "      <td>&lt;table&gt;&lt;caption&gt;table 6. ablation study on reg...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214030</th>\n",
       "      <td>2303.11749</td>\n",
       "      <td>2303.11749_60</td>\n",
       "      <td>detecting everything in the open world: toward...</td>\n",
       "      <td>&lt;table&gt;&lt;thead&gt;&lt;th&gt;decouple&lt;/th&gt;&lt;th&gt;proposal ge...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214031</th>\n",
       "      <td>2303.11749</td>\n",
       "      <td>2303.11749_61</td>\n",
       "      <td>detecting everything in the open world: toward...</td>\n",
       "      <td>pij = 1 1 + \\text{exp}_{}(−zt ijej/τ ) /πγ j ,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214032</th>\n",
       "      <td>2303.11749</td>\n",
       "      <td>2303.11749_62</td>\n",
       "      <td>detecting everything in the open world: toward...</td>\n",
       "      <td>references\\n[1] ankan bansal, karan sikka, gau...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214033 rows × 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T05:23:45.421030Z",
     "start_time": "2024-06-08T05:05:22.060493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = df[1500:1600]\n",
    "question = [google_gemini_api(title=row['title'], context=row['doc'], foundation_model='gemini-1.5-flash') for i, row in tqdm(df.iterrows(), total=len(df))]\n",
    "df['question'] = question\n",
    "df"
   ],
   "id": "49e838e291ada7a8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7abcfb5180804d0f98aaf154edcb4071"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/l_9fgbw95hl79rj750047d1c0000gn/T/ipykernel_4129/593946295.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['question'] = question\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "        paper_id         doc_id  \\\n",
       "1500  2308.13989  2308.13989_75   \n",
       "1501  2308.13989  2308.13989_76   \n",
       "1502  2308.13989  2308.13989_77   \n",
       "1503  2308.13989  2308.13989_78   \n",
       "1504  2308.13989  2308.13989_79   \n",
       "...          ...            ...   \n",
       "1595  2303.01999  2303.01999_71   \n",
       "1596  2303.01999  2303.01999_72   \n",
       "1597  2303.01999  2303.01999_74   \n",
       "1598  2303.01999  2303.01999_75   \n",
       "1599  2303.01999  2303.01999_76   \n",
       "\n",
       "                                                  title  \\\n",
       "1500  ldl: line distance functions for panoramic loc...   \n",
       "1501  ldl: line distance functions for panoramic loc...   \n",
       "1502  ldl: line distance functions for panoramic loc...   \n",
       "1503  ldl: line distance functions for panoramic loc...   \n",
       "1504  ldl: line distance functions for panoramic loc...   \n",
       "...                                                 ...   \n",
       "1595  unsupervised 3d shape reconstruction by part r...   \n",
       "1596  unsupervised 3d shape reconstruction by part r...   \n",
       "1597  unsupervised 3d shape reconstruction by part r...   \n",
       "1598  unsupervised 3d shape reconstruction by part r...   \n",
       "1599  unsupervised 3d shape reconstruction by part r...   \n",
       "\n",
       "                                                    doc  \\\n",
       "1500  line transformer-based approach based on yoon ...   \n",
       "1501  d. details on experimental setup\\nin this sect...   \n",
       "1502  0.7 0 o\\ncandidate pose search evaluation we c...   \n",
       "1503  for other regions. for the inversion network, ...   \n",
       "1504  image in the stanford 2d-3d-s dataset [4] and ...   \n",
       "...                                                 ...   \n",
       "1595  <table><caption>table 3. ablation for phase co...   \n",
       "1596  <table><caption>table 8. training target numbe...   \n",
       "1597  <table><thead><th>method</th><th>scd|</th><th>...   \n",
       "1598  <table><tr><td colspan=\"2\">1:</td></tr><tr><td...   \n",
       "1599  <table><thead><th>\\lgorithm 6 phase ii helper ...   \n",
       "\n",
       "                                               question  label  \n",
       "1500  How does the LDL approach utilize line transfo...   1500  \n",
       "1501  How does the LDL method handle illumination va...   1501  \n",
       "1502  How does LDL compare to NetVLAD in terms of ca...   1502  \n",
       "1503  How does the LDL approach differ from previous...   1503  \n",
       "1504  How does the line distance function (LDL) appr...   1504  \n",
       "...                                                 ...    ...  \n",
       "1595  What is the purpose of the \"borrow\" function i...   1595  \n",
       "1596  What is the impact of varying the number of tr...   1596  \n",
       "1597  How do the different phases of the unsupervise...   1597  \n",
       "1598  What is the purpose of the `swap` procedure in...   1598  \n",
       "1599  What is the purpose of the `filter()` procedur...   1599  \n",
       "\n",
       "[100 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>doc</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>2308.13989</td>\n",
       "      <td>2308.13989_75</td>\n",
       "      <td>ldl: line distance functions for panoramic loc...</td>\n",
       "      <td>line transformer-based approach based on yoon ...</td>\n",
       "      <td>How does the LDL approach utilize line transfo...</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>2308.13989</td>\n",
       "      <td>2308.13989_76</td>\n",
       "      <td>ldl: line distance functions for panoramic loc...</td>\n",
       "      <td>d. details on experimental setup\\nin this sect...</td>\n",
       "      <td>How does the LDL method handle illumination va...</td>\n",
       "      <td>1501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>2308.13989</td>\n",
       "      <td>2308.13989_77</td>\n",
       "      <td>ldl: line distance functions for panoramic loc...</td>\n",
       "      <td>0.7 0 o\\ncandidate pose search evaluation we c...</td>\n",
       "      <td>How does LDL compare to NetVLAD in terms of ca...</td>\n",
       "      <td>1502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>2308.13989</td>\n",
       "      <td>2308.13989_78</td>\n",
       "      <td>ldl: line distance functions for panoramic loc...</td>\n",
       "      <td>for other regions. for the inversion network, ...</td>\n",
       "      <td>How does the LDL approach differ from previous...</td>\n",
       "      <td>1503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>2308.13989</td>\n",
       "      <td>2308.13989_79</td>\n",
       "      <td>ldl: line distance functions for panoramic loc...</td>\n",
       "      <td>image in the stanford 2d-3d-s dataset [4] and ...</td>\n",
       "      <td>How does the line distance function (LDL) appr...</td>\n",
       "      <td>1504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>2303.01999</td>\n",
       "      <td>2303.01999_71</td>\n",
       "      <td>unsupervised 3d shape reconstruction by part r...</td>\n",
       "      <td>&lt;table&gt;&lt;caption&gt;table 3. ablation for phase co...</td>\n",
       "      <td>What is the purpose of the \"borrow\" function i...</td>\n",
       "      <td>1595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>2303.01999</td>\n",
       "      <td>2303.01999_72</td>\n",
       "      <td>unsupervised 3d shape reconstruction by part r...</td>\n",
       "      <td>&lt;table&gt;&lt;caption&gt;table 8. training target numbe...</td>\n",
       "      <td>What is the impact of varying the number of tr...</td>\n",
       "      <td>1596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>2303.01999</td>\n",
       "      <td>2303.01999_74</td>\n",
       "      <td>unsupervised 3d shape reconstruction by part r...</td>\n",
       "      <td>&lt;table&gt;&lt;thead&gt;&lt;th&gt;method&lt;/th&gt;&lt;th&gt;scd|&lt;/th&gt;&lt;th&gt;...</td>\n",
       "      <td>How do the different phases of the unsupervise...</td>\n",
       "      <td>1597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>2303.01999</td>\n",
       "      <td>2303.01999_75</td>\n",
       "      <td>unsupervised 3d shape reconstruction by part r...</td>\n",
       "      <td>&lt;table&gt;&lt;tr&gt;&lt;td colspan=\"2\"&gt;1:&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td...</td>\n",
       "      <td>What is the purpose of the `swap` procedure in...</td>\n",
       "      <td>1598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>2303.01999</td>\n",
       "      <td>2303.01999_76</td>\n",
       "      <td>unsupervised 3d shape reconstruction by part r...</td>\n",
       "      <td>&lt;table&gt;&lt;thead&gt;&lt;th&gt;\\lgorithm 6 phase ii helper ...</td>\n",
       "      <td>What is the purpose of the `filter()` procedur...</td>\n",
       "      <td>1599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T05:05:19.008425Z",
     "start_time": "2024-06-08T05:05:16.774749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_path = 'dataset_class/datafolder/arxiv_qa/total/1500_1600_test.csv'\n",
    "df.to_csv(output_path, index=False)"
   ],
   "id": "3218ae26d5e53f95",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T05:23:45.431274Z",
     "start_time": "2024-06-08T05:23:45.425329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def google_gemini_api(\n",
    "    prompt: str,\n",
    "    foundation_model: str = 'gemini-pro',\n",
    "    temperature: float = 0\n",
    ") -> str:\n",
    "    \"\"\" make Arxiv Questioning & Answering dataset function with Google AI Gemini API\n",
    "\n",
    "    As you run this function before, you must set up the your own Google API key for the Gemini API.\n",
    "    you can use the gemini-pro-api for free with the Google API key.\n",
    "\n",
    "    we will use the Zero-Shot Learning for generating the QA dataset from the given paper link.\n",
    "    Args:\n",
    "        prompt: str, input prompt for generating the question\n",
    "        foundation_model (str): The foundation model for extracting food ingredients from the given text,\n",
    "                                default is 'gemini-pro'\n",
    "        temperature (float): default 0.0, the temperature value for the diversity of the output text\n",
    "                             (if you set T < 1.0, the output text will be more deterministic, sharpening softmax dist)\n",
    "                             (if you set T > 1.0, the output text will be more diverse, flattening softmax dist)\n",
    "\n",
    "    References:\n",
    "        https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/tutorials/quickstart_colab.ipynb?hl=ko#scrollTo=HTiaTu6O1LRC\n",
    "        https://ai.google.dev/gemini-api/docs/models/gemini?hl=ko\n",
    "        https://ai.google.dev/gemini-api/docs/get-started/python?hl=ko&_gl=1*7ufqxk*_up*MQ..*_ga*MTk2ODk3NDQyNi4xNzE0OTIwMjcw*_ga_P1DBVKWT6V*MTcxNDkyMDI2OS4xLjAuMTcxNDkyMDI2OS4wLjAuOTQwNDMwMTE.\n",
    "        https://ai.google.dev/gemini-api/docs/quickstart?hl=ko&_gl=1*12k4ofq*_up*MQ..*_ga*MTk2ODk3NDQyNi4xNzE0OTIwMjcw*_ga_P1DBVKWT6V*MTcxNDkyMDI2OS4xLjAuMTcxNDkyMDI2OS4wLjAuOTQwNDMwMTE.\n",
    "        https://ai.google.dev/api/python/google/generativeai/GenerativeModel?_gl=1*1ajz3qu*_up*MQ..*_ga*MTk2ODk3NDQyNi4xNzE0OTIwMjcw*_ga_P1DBVKWT6V*MTcxNDkyNDAyOC4yLjAuMTcxNDkyNDAyOC4wLjAuMTkwOTQyMjU0#generate_content\n",
    "    \"\"\"\n",
    "    GOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY')\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "    model = genai.GenerativeModel(foundation_model)\n",
    "    generation_config = genai.types.GenerationConfig(\n",
    "        candidate_count=1,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    datasets = ''\n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            contents=prompt,\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "        datasets = response.text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    return datasets\n"
   ],
   "id": "db0043753eb46e6a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T05:23:50.679002Z",
     "start_time": "2024-06-08T05:23:45.432850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\" code cell for prompt testing \"\"\"\n",
    "\n",
    "query = f\"\"\"You're a question machine. Read the title and context given above and generate the right question based on given context. Here are some rules for generating the questions:\n",
    "1. Questions should also be able to capture the features or characteristics of a given context.\n",
    "2. The purpose of asking you to create questions is to create a dataset of question-document pairs.\n",
    "3. Please create with purpose and generate creative, informative, and diverse questions.\n",
    "4. Do not return questions that are too similar to each other, or too general.\n",
    "5. Please only return the question text, keep the number of questions between 1 and 5 with total length less than 100 tokens.\n",
    "6. If you want to ask multiple questions, please separate them with spaces without newlines.\"\"\"\n",
    "\n",
    "titles, contexts = df.title.tolist()[0:30], df.doc.tolist()[0:30]\n",
    "dataset = list(zip(titles, contexts))\n",
    "\n",
    "prompt = \"\"\n",
    "chunk_size = 30\n",
    "for i in range(0, len(dataset), chunk_size):\n",
    "    for j, data in enumerate(dataset[i:i+chunk_size]):\n",
    "        title, context = data\n",
    "        prompt += f\"\\n\\ntitle{j}:{title}\\ncontext{j}:{context}\"\n",
    "\n",
    "    prompt += query\n",
    "    print(prompt)\n",
    "    result = google_gemini_api(\n",
    "        prompt=prompt,\n",
    "        foundation_model=\"gemini-1.5-flash\"\n",
    "    )\n",
    "\n",
    "result"
   ],
   "id": "709863e977855713",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "title0:ldl: line distance functions for panoramic localization\n",
      "context0:line transformer-based approach based on yoon et al. [57], line transformer-based approach finds candidate poses attaining the most line matches with the query image, and refines poses using pnp-ransac. for establishing line matches, we first render nt × nr synthetic views from the point cloud where we set nt = 100 and nr = 216. then, the top k1 = 100 poses are selected whose netvlad [3] features are closest to the query image. this intermediate step is necessary as the line transformer features are com- putationally expensive and thus could not be naively evalu- ated for all nt × nr views. for each synthetic view from the selected poses, we extract line transformer embeddings and establish matchings with the query image. similar to the structure-based baseline, we convert panoramas to cube- maps during the line matching process. finally, we select the top k2 = 20 poses that have the most line matches, and refine them via pnp-ransac.\n",
      "\n",
      "title1:ldl: line distance functions for panoramic localization\n",
      "context1:d. details on experimental setup\n",
      "in this section, we provide additional details for experi- ments presented in section 4 and section b.\n",
      "illumination robustness evaluation to evaluate the ro- bustness of ldl against illumination shifts, we apply syn- thetic color variations to images in room 3 from om- niscenes [26]. we consider three synthetic color variations, where qualitative examples are shown in figure 4: average intensity, gamma, and white balance change. for average intensity change we lower each pixel intensity by 25%. for gamma change, we set the image gamma to 0.2. for white balance change, we apply the following transformation ma-\n",
      "0.7 0 o\n",
      "\n",
      "title2:ldl: line distance functions for panoramic localization\n",
      "context2:0.7 0 o\n",
      "candidate pose search evaluation we compare ldl against netvlad [3] for candidate pose search using the extreme split from omniscenes. the recall curves in fig- ure 5 are obtained by measuring the localization perfor- mance of both methods prior to pose refinement. as men- tioned in section 4.2, we use the identical set of translations with nt = 50 for both methods and associate a large num- ber of candidate rotations nr = 216 for netvlad to en- sure fair comparison. such measures are taken for rotations\n",
      "\n",
      "title3:ldl: line distance functions for panoramic localization\n",
      "context3:for other regions. for the inversion network, we use a sim- ilar u-net architecture as in ng et al. [34] where the only difference is in the input channel dimension that we set as 256 instead of 128 to match the superpoint [12] descriptor dimensions. then for training, we use the entire matter- port3d [9] dataset where we use the first 90% of the 9581 panorama images for training and the rest for validation. we follow the training procedure of ng et al. [34] and use the perceptual loss and mean absolute error (mae) loss, where we employ adam [28] with a learning rate of 1e−4 for op- timization. in our experiments, we use the trained network to reconstruct panoramas from the local feature descriptors, where we shared the reconstruction results along with the image error metrics in section 4 and section b. to elab- orate, during evaluation we first extract local features for each query image in the stanford 2d-3d-s dataset [4] and run feature inversion, where the results are then compared against the original\n",
      "\n",
      "title4:ldl: line distance functions for panoramic localization\n",
      "context4:image in the stanford 2d-3d-s dataset [4] and run feature inversion, where the results are then compared against the original panorama image.\n",
      "\n",
      "title5:ldl: line distance functions for panoramic localization\n",
      "context5:figure d.8. visualization of the 3d line segments used for ldl. while the line segment extraction algorithm from xiaohu et al. [54] can reliably extract the wireframe-like structure from the original 3d scan, the line segments are still quite noisy. note that we have cropped the ceilings of the original point cloud for better visualization.\n",
      "since ldl estimates rotations using combinatorial match- ings of principal directions, which makes the number of candidate rotations to vary for each query image. we empir- cally find that less than 30 candidate rotations remain after discarding infeasible rotations, and thus setting nr = 216 for netvlad would provide enough evidence to achieve competitive performance against ldl.\n",
      "\n",
      "title6:ldl: line distance functions for panoramic localization\n",
      "context6:3d line maps for localization in figure d.8, we show visualizations of 3d lines used as input to ldl. despite the reliabilty of the 3d line extraction algorithm of xiaohu et al. [54], the lines are still quite noisy. to cope with the noisy detections, ldl employs a length-based filtering scheme to only keep long, salient lines and resorts to matching the dis- tribution of lines using line distance functions instead of try- ing to establish direct one-to-one matchings as in previous works [33, 57].\n",
      "\n",
      "title7:ldl: line distance functions for panoramic localization\n",
      "context7:feature inversion network for privacy evaluation to evaluate the privacy protection of ldl against feature inver- sion attacks, we train a fully-convolutional neural network fθ(·) that takes a sparse feature map d ∈ rh×w ×c as input and produces image reconstructions. the feature map stores local feature descriptors f ∈ rc at keypoint loca- tions (ikpt, jkpt), namely d(ikpt, jkpt) = f , and zero values\n",
      "[6] christian cachin, idit keidar, and alexander shraer. trust- ing the cloud. sigact news, 40(2):81–86, jun 2009. 8, [7] dylan campbell, lars petersson, laurent kneip, and hong- dong li. globally-optimal inlier set maximisation for cam- era pose and correspondence estimation. ieee transactions on pattern analysis and machine intelligence, page preprint, june 2018. 1, 2, 5,\n",
      "\n",
      "title8:ldl: line distance functions for panoramic localization\n",
      "context8:[11] deeksha dangwal, vincent t. lee, hyo jin kim, tianwei shen, meghan cowan, rajvi shah, caroline trippel, bran- don reagen, timothy sherwood, vasileios balntas, armin alaghi, and eddy ilg. analysis and mitigations of reverse engineering attacks on local feature descriptors, 2021. 8, [12] daniel detone, tomasz malisiewicz, and andrew rabi- novich. superpoint: self-supervised interest point detection and description. in cvpr deep learning for visual slam workshop, 2018. 2, 3, 7,\n",
      "[19] geonmo gu, byungsoo ko, seounghyun go, sung-hyun lee, jingeun lee, and minchul shin. towards real-time and light-weight line segment detection. in proceedings of the aaai conference on artificial intelligence, 2022. 2, 6 [20] stephen hausler, sourav garg, ming xu, michael milford, and tobias fischer. patch-netvlad: multi-scale fusion of locally-global descriptors for place recognition. in proceed- ings of the ieee/cvf conference on computer vision and pattern recognition, pages 14141–14152, 2021. 7\n",
      "\n",
      "title9:ldl: line distance functions for panoramic localization\n",
      "context9:[23] martin humenberger, yohann cabon, nicolas guerin, julien morat, j´erˆome revaud, philippe rerole, no´e pion, cesar de souza, vincent leroy, and gabriela csurka. robust image retrieval-based visual localization using kapture, 2020. 2, 7 [24] martin humenberger, yohann cabon, nicolas gu´erin, julien morat, j´erˆome revaud, philippe rerole, no´e pion, c´esar roberto de souza, vincent leroy, and gabriela csurka. robust image retrieval-based visual localization us- ing kapture. corr, abs/2007.13867, 2020. 2\n",
      "[30] xiaotian li, shuzhe wang, yi zhao, jakob verbeek, and juho kannala. hierarchical scene coordinate classification and regression for visual localization. in cvpr, 2020. 2 [31] david g. lowe. distinctive image features from scale- invariant keypoints. international journal of computer vi- sion, 60:91–110, 2004. 2\n",
      "in- door a* pathfinding through an octree representation of isprs annals of photogrammetry, remote a point cloud. sensing and spatial information sciences, iv21:249–255, oct. 2016.\n",
      "\n",
      "title10:ldl: line distance functions for panoramic localization\n",
      "context10:ference on 3d vision (3dv), pages 195–204, los alamitos, ca, usa, oct 2017. ieee computer society. 3\n",
      "[50] pablo speciale, johannes sch¨onberger, sudipta sinha, and marc pollefeys. privacy preserving image queries for camera localization. in 2019 ieee/cvf international conference on computer vision (iccv), pages 1486–1496, 2019. 8 [51] hajime taira, masatoshi okutomi, torsten sattler, mircea cimpoi, marc pollefeys, josef sivic, tomas pajdla, and ak- ihiko torii. inloc: indoor visual localization with dense matching and view synthesis. in cvpr 2018 - ieee con- ference on computer vision and pattern recognition, salt lake city, united states, june 2018. 5, 8\n",
      "mation processing systems, volume 30. curran associates, inc., 2017. 2, 6\n",
      "\n",
      "title11:ldl: line distance functions for panoramic localization\n",
      "context11:mation processing systems, volume 30. curran associates, inc., 2017. 2, 6\n",
      "[58] xin yu, sagar chaturvedi, chen feng, yuichi taguchi, teng- yok lee, clinton fernandes, and srikumar ramalingam. vlase: vehicle localization by aggregating semantic edges. in 2018 ieee/rsj international conference on intelligent robots and systems (iros), pages 3196–3203, 2018. 2 [59] ziheng zhang, zhengxin li, ning bi, jia zheng, jinlei wang, kun huang, weixin luo, yanyu xu, and shenghua gao. ppgnet: learning point-pair graph for line segment in proceedings of the ieee/cvf conference detection. on computer vision and pattern recognition (cvpr), june 2019. 2, 6\n",
      "\n",
      "title12:ldl: line distance functions for panoramic localization\n",
      "context12:<table><caption>table 1. localization performance evaluation in stanford 2d-3d-s [4], compared against piccolo (pc) [26], structure-based approach (sb), chamfer distance-based approach (cd), line transformer (lt) [57], and cpo [27].</caption><thead><th rowspan=\"2\">dataset]</th><th colspan=\"6\">t-error (m)</th><th colspan=\"6\">r-error (°)</th><th colspan=\"6\">accuracy</th></thead><thead><th></th><th>pc</th><th>sb</th><th>cd</th><th>lt</th><th>cpo</th><th>ldl|</th><th>pc</th><th>cd</th><th>lt</th><th>cpo</th><th>ldl|</th><th>pc</th><th>sb</th><th>cd</th><th>lt</th><th>cpo</th><th>ldl</th></thead><tr><td>area 1</td><td>|0.02</td><td>0.02</td><td>0.12</td><td>0.02</td><td>0.01</td><td>0.02|</td><td>0.46</td><td>1.14</td><td>0.62</td><td>0.25</td><td></td><td>0.54|0.66</td><td>0.89</td><td>0.50</td><td>0.90</td><td>0.90</td><td>0.86</td></tr><tr><td>area 2</td><td>|0.76</td><td>0.04</td><td>1.16</td><td>0.04</td><td>0.01</td><td>0.02</td><td>| 2.25</td><td>11.54</td><td>0.72</td><td>0.27</td><td></td><td>0.66/0.45</td><td>0.76</td><td>0.35</td><td>0.74</td><td>0.81</td><td>0.77</td></tr><tr><td>area 3</td><td>|0.02</td><td>0.03</td><td>0.79</td><td>0.02</td><td>0.01</td><td>0.02]</td><td>0.49</td><td>4.54</td><td>0.55</td><td>0.24</td><td></td><td>0.54/0.57</td><td>0.92</td><td>0.36</td><td>0.88</td><td>0.78</td><td>0.89</td></tr><tr><td>area 4</td><td>|0.18</td><td>0.02</td><td>0.33</td><td>0.02</td><td>0.01</td><td>0.02)</td><td>4.17</td><td>1.97</td><td>0.56</td><td>0.28</td><td>0.48</td><td>/0.49</td><td>0.91</td><td>0.46</td><td>0.91</td><td>0.83</td><td>0.88</td></tr><tr><td>area 5</td><td>|0.50</td><td>0.03</td><td>0.95</td><td>0.03</td><td>0.01</td><td>0.02</td><td>| 14.64</td><td>41.84</td><td>0.65</td><td>0.27</td><td></td><td>0.54]0.44</td><td>0.80</td><td>0.36</td><td>0.79</td><td>0.74</td><td>0.81</td></tr><tr><td>area 6</td><td>|0.01</td><td>0.02</td><td>0.50</td><td>0.02</td><td>0.01</td><td>0.02]</td><td>0.31</td><td>1.20</td><td>0.60</td><td>0.18</td><td></td><td>0.50/0.69</td><td>0.88</td><td>0.47</td><td>0.87</td><td>0.90</td><td>0.83</td></tr><tr><td>total</td><td>[0.03</td><td>0.03</td><td>0.73</td><td>0.02</td><td>0.01</td><td>0.02</td><td>| 0.63</td><td>2.30</td><td>0.63</td><td>0.24</td><td>0.53</td><td>| 0.54</td><td>0.85</td><td>0.39</td><td>0.84</td><td>0.83</td><td>0.83</td></tr></table>\n",
      "\n",
      "title13:ldl: line distance functions for panoramic localization\n",
      "context13:<table><caption>table 2. localization accuracy on synthetic color variations ap- plied to room 3 in the extreme split from omniscenes [26].</caption><thead><th>dataset</th><th>pc</th><th>sb</th><th>cd</th><th>accuracy lt</th><th>cpo</th><th>ldl</th></thead><tr><td>original</td><td>0.45</td><td>0.69</td><td>0.21</td><td>0.68</td><td>0.72</td><td>0.89</td></tr><tr><td>gamma</td><td>0.00</td><td>0.63</td><td>0.47</td><td>0.59</td><td>0.00</td><td>0.82</td></tr><tr><td>intensity</td><td>0.00</td><td>0.56</td><td>0.40</td><td>0.58</td><td>0.80</td><td>0.76</td></tr><tr><td>white balance</td><td>|0.00</td><td>0.62</td><td>0.32</td><td>0.67</td><td>0.74</td><td>0.91</td></tr></table>\n",
      "\n",
      "title14:ldl: line distance functions for panoramic localization\n",
      "context14:<table><caption>table 3. localization performance evaluation in omniscenes [26], considering both scenes with and without object layout changes.</caption><thead><th rowspan=\"2\">split</th><th rowspan=\"2\">change|</th><th colspan=\"6\">t-error (m)</th><th colspan=\"6\">r-error (°)</th><th></th><th colspan=\"5\">accuracy</th></thead><thead><th></th><th></th><th>pc</th><th>sb</th><th>cd</th><th>lt</th><th>cpo</th><th>ldl|</th><th>pc</th><th>sb</th><th>cd</th><th>lt</th><th>cpo</th><th></th><th>ldl|}pc</th><th>sb</th><th>cd</th><th>lt</th><th>cpo</th><th>ldl</th></thead><tr><td>robot</td><td>x</td><td>|0.02</td><td>0.03</td><td>1.74</td><td>0.03</td><td>0.01</td><td>0.02)</td><td>0.27</td><td>0.58</td><td>89.23</td><td>0.59</td><td>0.12</td><td>0.49</td><td>|0.69</td><td>0.99</td><td>0.31</td><td>0.99</td><td>0.89</td><td>0.98</td></tr><tr><td>hand</td><td>x</td><td>|0.01</td><td>0.03</td><td>2.10</td><td>0.03</td><td>0.01</td><td>0.03]</td><td>0.23</td><td>0.63</td><td>89.02</td><td>0.64</td><td>0.13</td><td>0.54</td><td>|0.81</td><td>0.95</td><td>0.29</td><td>0.95</td><td>0.80</td><td>0.97</td></tr><tr><td>robot</td><td>v</td><td>1.07</td><td>0.04</td><td>1.78</td><td>0.04</td><td>0.02</td><td>0.03</td><td>|21.03</td><td>0.64</td><td>89.27</td><td>0.65</td><td>1.46</td><td></td><td>0.58|0.41</td><td>0.93</td><td>0.30</td><td>0.94</td><td>0.59</td><td>0.95</td></tr><tr><td>hand</td><td>¥</td><td>|0.53</td><td>0.04</td><td>1.70</td><td>0.04</td><td>0.02</td><td>0.03 |</td><td>7.54</td><td>0.71</td><td>88.50</td><td>0.70</td><td>0.37</td><td></td><td>0.64/0.47</td><td>0.92</td><td>0.30</td><td>0.90</td><td>0.60</td><td>0.92</td></tr><tr><td>extreme</td><td>v¥</td><td>1.24</td><td>0.04</td><td>1.55</td><td>0.04</td><td>0.03</td><td>0.03 |</td><td>23.71</td><td>0.83</td><td>88.54</td><td>0.84</td><td>0.37</td><td></td><td>0.72|0.41</td><td>0.89</td><td>0.29</td><td>0.88</td><td>0.59</td><td>0.92</td></tr></table>\n",
      "\n",
      "title15:ldl: line distance functions for panoramic localization\n",
      "context15:<table><caption>table b.1. ablation study of uniformly sampling query points on the unit sphere. ldl is compared against a variant using query points sampled along 2d line segment locations (ldlls) in the stanford 2d-3d-s dataset [4].</caption><tr><td rowspan=\"2\"></td><td>| terror</td><td>r-error</td><td></td><td>component</td><td>| cpu</td><td>gpu</td></tr><tr><td></td><td>(m)</td><td>@)</td><td></td><td>line segment extraction</td><td>0.141</td><td>0.141</td></tr><tr><td></td><td>0.06</td><td>1.18</td><td>0.63</td><td>rotation estimation</td><td>1.124</td><td>0,009</td></tr><tr><td>sb(k=20)</td><td>| 0.05</td><td>1.07</td><td>0.71</td><td>distance function computation</td><td>|0.052</td><td>0.001</td></tr><tr><td>ldl (k=10)|</td><td>0.07</td><td>1.36</td><td>0.63</td><td>candidate pose refinement</td><td>—_|5.573</td><td>0.587</td></tr><tr><td>ldl (k'=20)|</td><td>0.07</td><td>1.27</td><td>0.69</td><td>total runtime (sec)</td><td>[6.890</td><td>0.738</td></tr></table>\n",
      "\n",
      "title16:ldl: line distance functions for panoramic localization\n",
      "context16:<table><caption>table b.2. ablation study on the choice of loss functions evaluated in omniscenes [26].</caption><thead><th>method letho</th><th>terror im)</th><th>r-error sy @)</th><th>ace. a:</th><th>method</th><th>terror im)</th><th>r-error @)</th><th>oc as:</th></thead><tr><td>w/o decomposition]</td><td>1.00</td><td>3.97</td><td>0.37“</td><td>w/li loss</td><td>0.08</td><td>1.38</td><td>0.55</td></tr><tr><td>w/]q| = 10</td><td>0.04.</td><td>0.85</td><td>0.77.</td><td>w/l2loss</td><td>0.17</td><td>148</td><td>0.34</td></tr><tr><td>w/ |q| 1</td><td>0.04</td><td>0.71</td><td>«(0.88</td><td>~~ w/ huber loss |</td><td>0.11</td><td>139</td><td>0.50</td></tr><tr><td>w/ |q| = 84</td><td>0.03</td><td>0.66</td><td>0.95</td><td>w/ median loss}</td><td>0.08</td><td>1.22</td><td>0.55</td></tr><tr><td>ours (|q| = 42)</td><td>| 0.03</td><td>0.72</td><td>0.92</td><td>ours</td><td>0.07</td><td>1.22</td><td>0.68</td></tr></table>\n",
      "\n",
      "title17:ldl: line distance functions for panoramic localization\n",
      "context17:<table><caption>table b.4. evaluation on large scale scenes</caption><thead><th>area</th><th>|ldl</th><th>t-error(m) ldl's|ldl</th><th>| r-error</th><th>(°) ldl's|ldl</th><th></th><th>accuracy ldl's</th></thead><tr><td>area 1|</td><td>0.02</td><td>0.02</td><td>[0.54</td><td>0.60</td><td>|0.86</td><td>0.75</td></tr><tr><td>area2|0.02</td><td></td><td>0.05</td><td>|0.66</td><td>0.79</td><td>|0.77</td><td>0.57</td></tr><tr><td>area 3]</td><td>0.02</td><td>0.03</td><td>/0.54</td><td>0.73</td><td>|0.89</td><td>0.69</td></tr><tr><td>area4}0.02</td><td></td><td>0.02</td><td>0.48</td><td>0.57</td><td>| 0.88</td><td>0.72</td></tr><tr><td>area</td><td>5|0.02</td><td>0.03</td><td>/0.54</td><td>0.61</td><td>|0.81</td><td>0.59</td></tr><tr><td>area</td><td>6|0.02</td><td>0.02</td><td>|0.50</td><td>0.58</td><td>| 0.83</td><td>0.66</td></tr><tr><td>total |</td><td>0.02</td><td>0.03</td><td>| 0.53</td><td>0.64</td><td>| 0.83</td><td>0.66</td></tr></table>\n",
      "\n",
      "title18:ldl: line distance functions for panoramic localization\n",
      "context18:<table><thead><th>method</th><th>| t-error(m)</th><th>r-error (°)</th><th>acc.</th></thead><tr><td>w/ li loss</td><td>0.08</td><td>1.38</td><td>0.55</td></tr><tr><td>w/ l2 loss</td><td>0.17</td><td>1.48</td><td>0.34</td></tr><tr><td>w/ huber loss</td><td>0.11</td><td>1.39</td><td>0.50</td></tr><tr><td>w/ median loss</td><td>0.08</td><td>1.22</td><td>0.55</td></tr><tr><td>ours</td><td>| 0.07</td><td>1.22</td><td>0.68</td></tr></table>\n",
      "\n",
      "title19:ldl: line distance functions for panoramic localization\n",
      "context19:<table><thead><th>accuracy (0.2 m, 5°)</th><th>cpo</th><th>sb</th><th>lt cd</th><th>ldl</th></thead><tr><td>robot</td><td>0.70 0.89</td><td>1.00</td><td>1.00 0.34</td><td>0.99</td></tr><tr><td>hand</td><td>0.81 0.81</td><td>0.98</td><td>0.98 0.32</td><td>0.99</td></tr><tr><td>change robot</td><td>0.41 0.59</td><td>0.98</td><td>0.99 0.33</td><td>0.98</td></tr><tr><td>change hand</td><td>0.48 0.60</td><td>0.97</td><td>0.97 0.34</td><td>0.97</td></tr><tr><td>extreme</td><td>0.42 0.60</td><td>0.96</td><td>0.96 0.34</td><td>0.98</td></tr></table>\n",
      "\n",
      "title20:ldl: line distance functions for panoramic localization\n",
      "context20:<table><thead><th>accuracy (0.05 m, 5°)</th><th>pc cpo</th><th>sb lt cd</th><th>ldl</th></thead><tr><td>area |</td><td>0.66 0.89</td><td>0.83 0.83 0.46</td><td>0.83</td></tr><tr><td>area 2</td><td>0.42 0.81</td><td>0.63 0.63 0.30</td><td>0.69</td></tr><tr><td>area 3</td><td>0.53 0.76</td><td>0.81 0.82 0.34</td><td>0.86</td></tr><tr><td>area 4</td><td>0.48 0.83</td><td>0.87 0.88 0.43</td><td>0.85</td></tr><tr><td>area 5s</td><td>0.44 0.73</td><td>0.68 0.69 0.34</td><td>0.74</td></tr><tr><td>area 6</td><td>0.68 0.90</td><td>0.80 0.82 0.45</td><td>0.81</td></tr></table>\n",
      "\n",
      "title21:ldl: line distance functions for panoramic localization\n",
      "context21:<table><thead><th>accuracy (0.1 m, 5°) |</th><th>cpo</th><th>lt cd</th><th>ldl</th></thead><tr><td>area 1</td><td>0.90</td><td>0.90 0.50</td><td>0.86</td></tr><tr><td>area 2</td><td>0.81</td><td>0.74 0.35</td><td>0.77</td></tr><tr><td>area 3</td><td>0.78</td><td>0.88 0.36</td><td>0.89</td></tr><tr><td>area 4</td><td>0.83</td><td>0.91 0.46</td><td>0.88</td></tr><tr><td>area 5</td><td>0.74</td><td>0.79 0.36</td><td>0.81</td></tr><tr><td>area 6</td><td>0.90</td><td>0.87 0.47</td><td>0.83</td></tr></table>\n",
      "\n",
      "title22:ldl: line distance functions for panoramic localization\n",
      "context22:<table><thead><th>accuracy (0.1 m, 10°)|</th><th>cpo</th><th>lt cd</th></thead><tr><td>area |</td><td>0.66 0.90</td><td>0.89 0.90 0.50</td></tr><tr><td>area 2</td><td>0.45 0.81</td><td>0.76 0.74 0.35</td></tr><tr><td>area 3</td><td>0.57 0.78</td><td>0.92 0.88 0.36</td></tr><tr><td>area 4</td><td>0.49 0.83</td><td>0.91 0.91 0.46</td></tr><tr><td>area 5s</td><td>0.44 0.74</td><td>0.80 0.79 0.36</td></tr><tr><td>area 6</td><td>069 0.90</td><td>o88 o87 047</td></tr></table>\n",
      "\n",
      "title23:ldl: line distance functions for panoramic localization\n",
      "context23:<table><thead><th>accuracy (0.2 m, 5°) |</th><th>cpo</th><th>sb</th><th>lt cd</th><th>ldl</th></thead><tr><td>area 1</td><td>0.90</td><td>0.89</td><td>0.90 0.50</td><td>0.86</td></tr><tr><td>area 2</td><td>0.81</td><td>0.80</td><td>0.81 0.37</td><td>0.78</td></tr><tr><td>area 3</td><td>0.81</td><td>0.96</td><td>0.93 0.41</td><td>0.95</td></tr><tr><td>area 4</td><td>0.83</td><td>0.94</td><td>0.93 0.47</td><td>0.89</td></tr><tr><td>area 5</td><td>0.78</td><td>0.84</td><td>0.84 0.39</td><td>0.84</td></tr><tr><td>area 6</td><td>0.90</td><td>0.88</td><td>0.88 0.48</td><td>0.84</td></tr></table>\n",
      "\n",
      "title24:ldl: line distance functions for panoramic localization\n",
      "context24:<table><thead><th>accuracy (0.2 m, 10°)|</th><th>cpo</th><th>sb</th><th>lt cd</th><th>ldl</th></thead><tr><td>area |</td><td>0.90</td><td>0.89</td><td>0.90 0.50</td><td>0.86</td></tr><tr><td>area 2</td><td>0.81</td><td>0.80</td><td>0.81 0.37</td><td>0.78</td></tr><tr><td>area 3</td><td>0.81</td><td>0.96</td><td>0.93 0.41</td><td>0.95</td></tr><tr><td>area 4</td><td>0.83</td><td>0.94</td><td>0.93 0.47</td><td>0.89</td></tr><tr><td>area 5s</td><td>0.78</td><td>0.84</td><td>0.84 0.39</td><td>0.84</td></tr><tr><td>area 6</td><td>0.90</td><td>0.88</td><td>0.88 0.48</td><td>0.84</td></tr></table>\n",
      "\n",
      "title25:ldl: line distance functions for panoramic localization\n",
      "context25:s × e ∥s × e∥ \\text{min}_{}(cos−1⟨x, e⟩, cos−1⟨x, s⟩) sin−1 |⟨x, ⟩| if x ∈ \\text{q}_{}(s, e) \\text{d}_{}(x, l) =\n",
      "\n",
      "title26:ldl: line distance functions for panoramic localization\n",
      "context26:\\text{f3d}_{}(x; l3d, r, t) = min ˜l∈l3d \\text{d}_{}(x, \\text{πl}_{}(˜l; r, t)). (3)\n",
      "\n",
      "title27:ldl: line distance functions for panoramic localization\n",
      "context27:\\text{l}_{}(ret)=-s 27 fon (al) fav (al 4p rd) <7}. i=1qeq (5)\n",
      "\n",
      "title28:ldl: line distance functions for panoramic localization\n",
      "context28:references\n",
      "[1] matterport 3d: how long does it take to scan a prop- erty? https://support.matterport.com/hc/ en-us/articles/229136307-how-long-does- it-take-to-scan-a-property-. accessed: 2020- 02-18.\n",
      "[2] hichem abdellali, robert frohlich, viktor vilagos, and zoltan kato. l2d2: learnable line detector and descriptor. in 2021 international conference on 3d vision (3dv), pages 442–452, 2021. 2\n",
      "[3] r. arandjelovi´c, p. gronat, a. torii, t. pajdla, and j. sivic. netvlad: cnn architecture for weakly supervised place in ieee conference on computer vision and recognition. pattern recognition, 2016. 2, 7,\n",
      "[4] iro armeni, sasha sax, amir r zamir, and silvio savarese. joint 2d-3d-semantic data for indoor scene understanding. arxiv preprint arxiv:1702.01105, 2017. 5, 6, 8,\n",
      "[5] eric brachmann, alexander krull, sebastian nowozin, j. shotton, f. michel, s. gumhold, and c. rother. dsac — differentiable ransac for camera localization. 2017 ieee conference on computer vision and pattern recognition (cvpr), pages 2492–2500, 2017. 2\n",
      "[8] dylan campbell, lars petersson, laurent kneip, hongdong li, and stephen gould. the alignment of the spheres: globally-optimal spherical mixture alignment for camera in proceedings of the 2019 ieee/cvf pose estimation. conference on computer vision and pattern recognition (cvpr), page to appear, long beach, usa, june 2019. ieee. 1, 2, 5,\n",
      "[9] angel chang, angela dai, thomas funkhouser, maciej hal- ber, matthias niessner, manolis savva, shuran song, andy zeng, and yinda zhang. matterport3d: learning from rgb- d data in indoor environments. international conference on 3d vision (3dv), 2017.\n",
      "[10] gabriela csurka and martin humenberger. features. handcrafted to deep local abs/1807.10254, 2018. 2 invariant from corr,\n",
      "[13] mihai dusmanu, ignacio rocco, tomas pajdla, marc polle- feys, josef sivic, akihiko torii, and torsten sattler. d2-net: a trainable cnn for joint detection and description of lo- in proceedings of the 2019 ieee/cvf con- cal features. ference on computer vision and pattern recognition, 2019. 7\n",
      "[14] mihai dusmanu, johannes sch¨onberger, sudipta sinha, and marc pollefeys. privacy-preserving image features via ad- in computer vision versarial affine subspace embeddings. and pattern recognition (cvpr 2021). cvf/ieee, june 2021. 8\n",
      "[15] martin a. fischler and robert c. bolles. random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. commun. acm, 24(6):381–395, 1981. 1, 2, 3, 5,\n",
      "[16] shuang gao, jixiang wan, yishan ping, xudong zhang, shuzhou dong, jijunnan li, and yandong guo. pose refine- ment with joint optimization of visual points and lines. 2022 ieee/rsj international conference on intelligent robots and systems (iros), pages 2888–2894, 2021. 2\n",
      "[17] yixiao ge, haibo wang, feng zhu, rui zhao, and hong- sheng li. self-supervising fine-grained region similarities for large-scale image localization. in european conference on computer vision, 2020. 2, 5, 7\n",
      "[18] rafael grompone von gioi, jeremie jakubowicz, jean- michel morel, and gregory randall. lsd: a fast line ieee segment detector with a false detection control. transactions on pattern analysis and machine intelligence, 32(4):722–732, 2010. 2, 3, 6\n",
      "[21] j. a. hesch and s. i. roumeliotis. a direct least-squares (dls) method for pnp. in 2011 international conference on computer vision, pages 383–390, 2011. 5\n",
      "[22] manuel hofer, michael maurer, and horst bischof. efficient 3d scene abstraction using line segments. computer vision and image understanding, 157:167–178, 2017. large-scale 3d modeling of urban indoor or outdoor scenes from im- ages and range scans.\n",
      "[25] w. kabsch. a solution for the best rotation to relate two sets of vectors. acta crystallographica section a, 32(5):922– 923, sep 1976. 4\n",
      "[26] junho kim, changwoon choi, hojun jang, and young min kim. piccolo: point cloud-centric omnidirectional localiza- tion. in proceedings of the ieee/cvf international confer- ence on computer vision (iccv), pages 3313–3323, october 2021. 1, 2, 5, 6, 7,\n",
      "[27] junho kim, hojun jang, changwoon choi, and young min kim. cpo: change robust panorama to point cloud localiza- tion. eccv, 2022. 1, 2, 5, 6,\n",
      "[28] diederik p. kingma and jimmy ba. adam: a method for stochastic optimization. in yoshua bengio and yann lecun, editors, 3rd international conference on learning represen- tations, iclr 2015, san diego, ca, usa, may 7-9, 2015, conference track proceedings, 2015.\n",
      "[29] vincent lepetit, francesc moreno-noguer, and pascal fua. epnp: an accurate o(n) solution to the pnp problem. inter- national journal of computer vision, 81:155–166, 2009. 1, 2, 5\n",
      "[32] branislav micusik and horst wildenauer. structure from motion with line segments under relaxed endpoint con- straints. in 2014 2nd international conference on 3d vision, volume 1, pages 13–19, 2014. 3\n",
      "[33] branislav micusik and horst wildenauer. descriptor free vi- sual indoor localization with line segments. in 2015 ieee conference on computer vision and pattern recognition (cvpr), pages 3165–3173, 2015. 2, 4, 6,\n",
      "[34] tony ng, hyo jin kim, vincent t. lee, daniel detone, tsun-yi yang, tianwei shen, eddy ilg, vassileios bal- ntas, krystian mikolajczyk, and chris sweeney. ninjadesc: content-concealing visual descriptors via adversarial learn- ing. in proceedings of the ieee/cvf conference on com- puter vision and pattern recognition (cvpr), pages 12797– 12807, june 2022. 8,\n",
      "[35] adam paszke, sam gross, francisco massa, adam lerer, james bradbury, gregory chanan, trevor killeen, zeming lin, natalia gimelshein, luca antiga, alban desmaison, andreas kopf, edward yang, zachary devito, martin rai- son, alykhan tejani, sasank chilamkurthy, benoit steiner, lu fang, junjie bai, and soumith chintala. pytorch: an im- perative style, high-performance deep learning library. in h. wallach, h. larochelle, a. beygelzimer, f. d'alch´e-buc, e. fox, and r. garnett, editors, advances in neural informa- tion processing systems 32, pages 8024–8035. curran asso- ciates, inc., 2019. 5\n",
      "[36] n. pion, m. humenberger, g. csurka, y. cabon, and t. sat- tler. benchmarking image retrieval for visual localization. in 2020 international conference on 3d vision (3dv), pages 483–494, los alamitos, ca, usa, nov 2020. ieee com- puter society. 2\n",
      "[37] francesco pittaluga, sanjeev j koppal, sing bing kang, and sudipta n sinha. revealing scenes by inverting structure in proceedings of the ieee from motion reconstructions. conference on computer vision and pattern recognition, pages 145–154, 2019. 8,\n",
      "[39] yohann sala¨un, renaud marlet, and pascal monasse. ro- bust and accurate line- and/or point-based pose estimation without manhattan assumptions. in european conference on computer vision, 2016. 3\n",
      "[40] y. salaun, r. marlet, and p. monasse. line-based robust in 2017 international con-\n",
      "[41] paul-edouard sarlin, cesar cadena, roland siegwart, and marcin dymczyk. from coarse to fine: robust hierarchical localization at large scale. corr, abs/1812.03506, 2018. 2\n",
      "[42] paul-edouard sarlin, daniel detone, tomasz malisiewicz, learning fea- corr, and andrew rabinovich. ture matching with graph neural networks. abs/1911.11763, 2019. 2 superglue:\n",
      "[43] paul-edouard sarlin, cesar cadena, roland siegwart, and marcin dymczyk. from coarse to fine: robust hierarchical localization at large scale. in cvpr, 2019. 2, 5, 7\n",
      "[44] paul-edouard sarlin, daniel detone, tomasz malisiewicz, superglue: learning feature and andrew rabinovich. matching with graph neural networks. in cvpr, 2020. 1, 2, 3, 5, 7,\n",
      "[45] torsten sattler, bastian leibe, and leif kobbelt. improving image-based localization by active correspondence search. in andrew fitzgibbon, svetlana lazebnik, pietro perona, yoichi sato, and cordelia schmid, editors, computer vision – eccv 2012, pages 752–765, berlin, heidelberg, 2012. springer berlin heidelberg. 2\n",
      "[46] torsten sattler, bastian leibe, and leif kobbelt. effi- cient & effective prioritized matching for large-scale image- based localization. ieee trans. pattern anal. mach. intell., 39(9):1744–1756, 2017. 2\n",
      "[47] johannes l. schonberger, hans hardmeier, torsten sattler, and marc pollefeys. comparative evaluation of hand-crafted in proceedings of the ieee and learned local features. conference on computer vision and pattern recognition (cvpr), july 2017. 2\n",
      "[48] j. shotton, b. glocker, c. zach, s. izadi, a. criminisi, and a. fitzgibbon. scene coordinate regression forests for cam- era relocalization in rgb-d images. in 2013 ieee conference on computer vision and pattern recognition, pages 2930– 2937, 2013. 2\n",
      "[49] pablo speciale, johannes l. sch¨onberger, sing bing kang, sudipta n. sinha, and marc pollefeys. privacy preserving image-based localization. 2019. 8\n",
      "[52] felix taubner, florian tschopp, tonci novkovic, roland siegwart, and fadri furrer. lcd – line clustering and de- scription for place recognition, 2020. 2\n",
      "[53] ashish vaswani, noam shazeer, niki parmar, jakob uszko- reit, llion jones, aidan n gomez, ł ukasz kaiser, and il- in i. guyon, lia polosukhin. attention is all you need. u. v. luxburg, s. bengio, h. wallach, r. fergus, s. vish- wanathan, and r. garnett, editors, advances in neural infor-\n",
      "[54] lu xiaohu, liu yahui, and li kai. fast 3d line segment arxiv preprint detection from unorganized point cloud. arxiv:1901.02532, 2019. 3,\n",
      "[55] nan xue, song bai, fudong wang, gui-song xia, tianfu wu, and liangpei zhang. learning attraction field represen- tation for robust line segment detection. in ieee conference on computer vision and pattern recognition (cvpr), 2019. 2, 6\n",
      "[56] w. yang, y. qian, j. k¨am¨ar¨ainen, f. cricri, and l. fan. ob- in 2018 24th ject detection in equirectangular panorama. international conference on pattern recognition (icpr), pages 2190–2195, aug 2018.\n",
      "[57] sungho yoon and ayoung kim. line as a visual sentence: context-aware line descriptor for visual localization. ieee robotics and automation letters, 6(4):8726–8733, 2021. 2, 4, 5, 6, 7,\n",
      "\n",
      "title29:unsupervised 3d shape reconstruction by part retrieval and assembly\n",
      "context29:r a\n",
      "\n",
      "unsupervised 3d shape reconstruction by part retrieval and assembly\n",
      "\n",
      "xianghao xu brown university usa\n",
      "paul guerrero adobe research uk matthew fisher adobe research usa siddhartha chaudhuri adobe research india\n",
      "\n",
      "daniel ritchie brown university usaYou're a question machine. Read the title and context given above and generate the right question based on given context. Here are some rules for generating the questions:\n",
      "1. Questions should also be able to capture the features or characteristics of a given context.\n",
      "2. The purpose of asking you to create questions is to create a dataset of question-document pairs.\n",
      "3. Please create with purpose and generate creative, informative, and diverse questions.\n",
      "4. Do not return questions that are too similar to each other, or too general.\n",
      "5. Please only return the question text, keep the number of questions between 1 and 5 with total length less than 100 tokens.\n",
      "6. If you want to ask multiple questions, please separate them with spaces without newlines.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'How does the proposed method for unsupervised 3D shape reconstruction differ from existing approaches? What are the key challenges in retrieving and assembling 3D shape parts in an unsupervised manner? What types of 3D shapes can be effectively reconstructed using this part retrieval and assembly approach? What are the potential applications of this unsupervised 3D shape reconstruction method?  What are the limitations of the proposed method for unsupervised 3D shape reconstruction? \\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T07:20:03.252346Z",
     "start_time": "2024-06-01T07:18:54.782275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\" code cell 2 for second version of prompt testing\n",
    "\n",
    "first ver: title, context\n",
    "second ver: context\n",
    "\"\"\"\n",
    "\n",
    "query = \"\"\"You're a question machine.\\nThe given text has a number of contexts numbered. Create questions that are appropriate for each context.\\nQuestions should capture the features or characteristics of the given context.\\nThe purpose of asking you to create questions is to create a dataset of question-document pairs.\\nPlease create with purpose and generate creative, informative, and meaningful questions.\\nDo not return questions that are too similar to each other or too general.\\nPlease only return the questions' text, and the number of questions should be between 1 and 5 per single individual context group. If you want to ask more than one questions about single context, please separate them with space, not newlines.\\nEach context's questions should be no more than 100 tokens.\\nSeparate questions for different context group with line breaks.\"\"\"\n",
    "\n",
    "\n",
    "dataset = df.doc.tolist()[0:10]\n",
    "\n",
    "prompt = \"\"\n",
    "chunk_size = 10\n",
    "for i in range(0, len(dataset), chunk_size):\n",
    "    for j, data in enumerate(dataset[i:i+chunk_size]):\n",
    "        prompt += f\"[context{j}]\\n{data}\\n\\n\"\n",
    "\n",
    "    prompt += query\n",
    "    print(prompt)\n",
    "    result = google_gemini_api(\n",
    "        prompt=prompt,\n",
    "        foundation_model=\"gemini-1.5-flash\"\n",
    "    )\n",
    "\n",
    "result"
   ],
   "id": "72e6a2ccc66a3644",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[context0]\n",
      "Evaluating the Ripple Effects of Knowledge Editing in Language Models\n",
      "Roi Cohen1 Eden Biran1 Ori Yoran1 Amir Globerson1,2 Mor Geva1,2, 1Blavatnik School of Computer Science, Tel Aviv University 2Google Research {roi1, edenbiran, oriy}@mail.tau.ac.il, {gamir, morgeva}@tauex.tau.ac.il\n",
      "\n",
      "[context1]\n",
      "Abstract\n",
      "\n",
      "[context2]\n",
      "Modern language models capture a large body of factual knowledge. However, some facts can be incorrectly induced or become obsolete over time, resulting in factually in- correct generations. This has led to the de- velopment of various editing methods that allow updating facts encoded by the model. Evaluation of these methods has primarily focused on testing whether an individual fact has been successfully injected, and if sim- ilar predictions for other subjects have not changed. Here we argue that such evaluation is limited, since injecting one fact (e.g. “Jack Depp is the son of Johnny Depp”) introduces a “ripple effect” in the form of additional facts that the model needs to update (e.g., “Jack Depp is the sibling of Lily-Rose Depp”). To address this, we propose novel evaluation criteria that consider the implications of an edit on related facts. Using these criteria, we then construct RIPPLEEDITS, a diagnos- tic benchmark of 5K factual edits, capturing various types of ripple effects. We evaluate\n",
      "\n",
      "[context3]\n",
      "construct RIPPLEEDITS, a diagnos- tic benchmark of 5K factual edits, capturing various types of ripple effects. We evaluate prominent editing methods on RIPPLEED- ITS, showing that they fail to introduce con- sistent changes in the model’s knowledge. In addition, we find that a simple in-context editing baseline obtains the best scores on our benchmark, suggesting a promising re- search direction for model editing.1\n",
      "\n",
      "[context4]\n",
      "Figure 1: Illustration of the evaluation scope of RIP- PLEEDITS, compared to existing knowledge editing benchmarks. For a given factual edit, we consider the “ripple effect” of the edit on the model’s knowledge.\n",
      "\n",
      "[context5]\n",
      "Introduction\n",
      "model may be incorrect or become outdated over time, potentially affecting the model’s performance on downstream tasks, its reliability and its usability (Dhingra et al., 2022; Lazaridou et al., 2021; Jang et al., 2022).\n",
      "Modern language models (LMs) capture a large volume of factual knowledge in their parameters, which can be effectively utilized in downstream tasks (Petroni et al., 2019; Roberts et al., 2020; Shin et al., 2020; Razniewski et al., 2021; Heinzerling and Inui, 2021; Kadavath et al., 2022; Cohen et al., 2023a). However, factual beliefs captured by the\n",
      "∗Work done at Google DeepMind. 1We release RIPPLEEDITS and our code at https://\n",
      "github.com/edenbiran/RippleEdits.\n",
      "\n",
      "[context6]\n",
      "∗Work done at Google DeepMind. 1We release RIPPLEEDITS and our code at https://\n",
      "github.com/edenbiran/RippleEdits.\n",
      "This limitation has prompted research on knowl- edge editing (KE) methods, which modify LMs to fix their factual errors (we provide a formal defini- tion in §2). Knowledge editing work has focused on applying factual updates to LMs. Given an entity-relation-object triplet (e, r, o) representing a fact (e.g. “Lionel Messi plays for the Inter Mi- ami team”), recent work proposed various methods (Mitchell et al., 2022a; Meng et al., 2022, 2023; Hernandez et al., 2023b; Si et al., 2023) to inject\n",
      "\n",
      "[context7]\n",
      "this fact into the parameters of a given LM, while “overriding” beliefs the model might have on e and r (e.g. that Messi plays for Paris Saint-Germain). A key question with KE is how to evaluate the success of such editing operations. The most basic “sanity-check” is that the model correctly completes (e, r, ?), as well as other paraphrases of this task, with o. However, this is not enough as an evalua- tion, since one needs to check that the model did not distort other facts. Indeed, the standard evalua- tion protocol (Mitchell et al., 2022b; Meng et al., 2022, 2023) for KE focuses on these two aspects of correctly completing various paraphrases of the new fact, as well as ensuring that other unrelated facts have not been changed.\n",
      "\n",
      "[context8]\n",
      "In this work, we argue that to evaluate model edits, one should go beyond the single fact that was edited and check that other facts that are logically derived from the edit were also changed accord- ingly. For example, if z is the mother of e, then the children of z are the siblings of e. Consequently, once we modify the belief of a certain model that z → z′ is the mother of e, then we should also en- sure that the model’s belief regarding the siblings of e is also correct. Fig. 1 illustrates another ex- ample, where editing the Team for which Lionel Messi plays modifies other related facts, such as his country of residence, while other facts should be retained. We refer to such changes that are im- plied by a factual edit as “ripple effects”.\n",
      "\n",
      "[context9]\n",
      "To account for ripple effects in the evaluation of factual edits, we propose six concrete evaluation criteria (see §3, Fig. 2), for testing which facts other than the edit itself should be modified or retained post-editing. Our tests evaluate how well the model integrates the edit with the rest of its knowledge, through queries that involve logical reasoning, complex composition of facts with the edit as an intermediate step, subject aliasing, and specificity across relations.\n",
      "Building upon these criteria, we create RIP- PLEEDITS, a new benchmark for comprehensive evaluation of KE of LMs (see §4). RIPPLEEDITS includes 5K entries, each consisting of a factual edit, along with a set of test queries that check if the edit was successful in terms of its ripple effect. Moreover, RIPPLEEDITS contains meta-data for each edit, including information about the times- tamp of the edit (i.e., recent versus old), and the popularity of the entities (i.e., head versus tail).\n",
      "We use RIPPLEEDITS to evaluate three popular\n",
      "\n",
      "You're a question machine.\n",
      "The given text has a number of contexts numbered. Create questions that are appropriate for each context.\n",
      "Questions should capture the features or characteristics of the given context.\n",
      "The purpose of asking you to create questions is to create a dataset of question-document pairs.\n",
      "Please create with purpose and generate creative, informative, and meaningful questions.\n",
      "Do not return questions that are too similar to each other or too general.\n",
      "Please only return the questions' text, and the number of questions should be between 1 and 5 per single individual context group. If you want to ask more than one questions about single context, please separate them with space, not newlines.\n",
      "Each context's questions should be no more than 100 tokens.\n",
      "Separate questions for different context group with line breaks.\n",
      "504 Deadline Exceeded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
